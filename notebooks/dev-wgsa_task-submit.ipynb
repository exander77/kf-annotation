{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation task submission notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sevenbridges as sbg\n",
    "from sevenbridges.errors import SbgError\n",
    "from sevenbridges.http.error_handlers import rate_limit_sleeper, maintenance_sleeper\n",
    "import sys\n",
    "import re\n",
    "import pdb\n",
    "import concurrent.futures\n",
    "from requests import request\n",
    "config = sbg.Config(profile='turbo')\n",
    "api = sbg.Api(config=config, error_handlers=[rate_limit_sleeper, maintenance_sleeper])\n",
    "project = 'd3b-bixu/dev-wgsa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draft task def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_task(task_name, input_dict, app_name, project):\n",
    "    task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=input_dict, run=False)\n",
    "    task.inputs['output_basename'] = task.id\n",
    "    task.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run task def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tasks(project, prefix):\n",
    "    # set prefix to all if you don't need to be selective\n",
    "    draft_tasks = list(api.tasks.query(project=project, status='DRAFT').all())\n",
    "    for i in range(0, len(draft_tasks), 1):\n",
    "        if prefix == 'ALL' or re.search(prefix, draft_tasks[i].name):\n",
    "            draft_tasks[i].run()\n",
    "            print('Running task ' + draft_tasks[i].id + ' ' + draft_tasks[i].name)\n",
    "        else:\n",
    "            print('Task ' + draft_tasks[i].id + ' ' + draft_tasks[i].name + ' skipped, prefix ' + prefix + ' did not match')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### execute run tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'KF ANNOT UNK VAR'\n",
    "print(\"You sure you want to run all tasks with prefix: \" + prefix + \"? Type \\\"YASS\\\" if so\")\n",
    "check = input()\n",
    "if check == \"YASS\":\n",
    "    run_tasks(project, prefix)\n",
    "else:\n",
    "    sys.stderr.write(\"User did not type YASS, skipping\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove old annotation task set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = open('/Users/brownm28/Documents/2020-Mar-4_WGSA/TASK_RUN/original_trio_vcf_test-manifest.csv')\n",
    "head = next(manifest)\n",
    "app = project + '/bcftools-strip-info'\n",
    "strip_info = 'INFO/ANN'\n",
    "tool_name = 'gatk.denovo.trio'\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    in_dict['input_vcf'] = api.files.get(info[0])\n",
    "    in_dict['tool_name'] = tool_name\n",
    "    in_dict['strip_info'] = strip_info\n",
    "    task_name = \"BCFTOOLS STRIP ANNO: \" + in_dict['input_vcf'].metadata['Kids First Family ID']\n",
    "    draft_task(task_name, in_dict, app, project)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy metadata to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metadata_to_outputs(task, phrase, in_key):\n",
    "    if re.search(phrase, task.name):\n",
    "        sys.stderr.write('Valid task found ' + task.name + '\\n')\n",
    "        metadata = task.inputs[in_key].metadata\n",
    "        for out_key in task.outputs:\n",
    "            # pdb.set_trace()\n",
    "            try:\n",
    "                if type(task.outputs[out_key]) is not list:\n",
    "                    file_obj = api.files.get(task.outputs[out_key].id)\n",
    "                    for key in metadata:\n",
    "                        file_obj.metadata[key] = metadata[key]\n",
    "                    file_obj.save()\n",
    "                else:\n",
    "                    for output in task.outputs[out_key]:\n",
    "                        if type(output) is not list:\n",
    "                            file_obj = api.files.get(output.id)\n",
    "                            for key in metadata:\n",
    "                                file_obj.metadata[key] = metadata[key]\n",
    "                            file_obj.save()\n",
    "                        else:\n",
    "                            for item in output:\n",
    "                                if item is not None:\n",
    "                                    file_obj = api.files.get(item.id)\n",
    "                                    for key in metadata:\n",
    "                                        file_obj.metadata[key] = metadata[key]\n",
    "                                    file_obj.save()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Skipping \" + out_key + \" for \" + task.name + \" due to error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add metadata to file normal tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'KF ANNOT UNK VAR'\n",
    "key = 'input_vcf'\n",
    "print(\"You sure tag outputs with task prefix: \" + prefix + \"? Type \\\"YASS\\\" if so\")\n",
    "check = input()\n",
    "if check == \"YASS\":\n",
    "    tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "    for task in tasks:\n",
    "        add_metadata_to_outputs(task, prefix, key)\n",
    "else:\n",
    "    sys.stderr.write(\"User did not type YASS, skipping\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add file tag to task outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_file_outputs(task, phrase, tags):\n",
    "    if re.search(phrase, task.name):\n",
    "        sys.stderr.write('Valid task found ' + task.name + '\\n')\n",
    "        for out_key in task.outputs:\n",
    "            # pdb.set_trace()\n",
    "            try:\n",
    "                if type(task.outputs[out_key]) is not list:\n",
    "                    file_obj = api.files.get(task.outputs[out_key].id)\n",
    "                    file_obj.tags = tags\n",
    "                    file_obj.save()\n",
    "                else:\n",
    "                    for output in task.outputs[out_key]:\n",
    "                        if type(output) is not list:\n",
    "                            file_obj = api.files.get(output.id)\n",
    "                            file_obj.tags = tags\n",
    "                            file_obj.save()\n",
    "                        else:\n",
    "                            for item in output:\n",
    "                                if item is not None:\n",
    "                                    file_obj = api.files.get(item.id)\n",
    "                                    file_obj.tags = tags\n",
    "                                    file_obj.save()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"Skipping \" + out_key + \" for \" + task.name + \" due to error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'MARAZITA CALLER ONLY TEST'\n",
    "tags = ['ANNOTATED', 'CALLER_ONLY_ALL_VAR']\n",
    "print(\"You sure tag outputs with task prefix: \" + prefix + \"? Type \\\"YASS\\\" if so\")\n",
    "check = input()\n",
    "if check == \"YASS\":\n",
    "    tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "    for task in tasks:\n",
    "        tag_file_outputs(task, prefix, tags)\n",
    "else:\n",
    "    sys.stderr.write(\"User did not type YASS, skipping\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tag batch tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'bcftools-filter-vcf run - RM SNP'\n",
    "key = 'input_vcf'\n",
    "print(\"You sure tag outputs with task prefix: \" + prefix + \"? Type \\\"YASS\\\" if so\")\n",
    "check = input()\n",
    "if check == \"YASS\":\n",
    "    tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "    for task in tasks:\n",
    "        if re.search(prefix, task.name):\n",
    "            for child in task.get_batch_children():\n",
    "                tag_outputs(child, prefix, key)\n",
    "else:\n",
    "    sys.stderr.write(\"User did not type YASS, skipping\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete task outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'WGSA SNP INDEL ALL'\n",
    "print(\"You sure you want to delete outputs from tasks with prefix: \" + prefix + \"? Type \\\"YASS\\\" if so\")\n",
    "check = input()\n",
    "if check == \"YASS\":\n",
    "    tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "    for task in tasks:\n",
    "        try:\n",
    "            if re.search(prefix, task.name):\n",
    "                for key in task.outputs:\n",
    "                    if isinstance(task.outputs[key], list):\n",
    "                        for f_obj in task.outputs[key]:\n",
    "                            f_obj.delete()\n",
    "                    elif task.outputs[key] is not None:\n",
    "                        task.outputs[key].delete()\n",
    "        except Exception as e:\n",
    "            sys.stderr.write(str(e) + \"\\nError processing \" + task.name + \"task id: \" + task.id + \". Review error if ok\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_vcf_manifest = '/Users/brownm28/Documents/2020-Mar-4_WGSA/TASK_RUN/stripped_vcf-manifest.csv'\n",
    "snp_rm_vcf_manifest = '/Users/brownm28/Documents/2020-Mar-4_WGSA/TASK_RUN/snp_rm-manifest.csv'\n",
    "subset_test = '/Users/brownm28/Documents/2020-Mar-4_WGSA/TASK_RUN/subset_test-manifest.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up snpEff run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snpEff_refs(reference_name, tool_name):\n",
    "    ref_dict = {}\n",
    "    ref_dict['reference_name'] = reference_name\n",
    "    ref_dict['ref_tar_gz'] = api.files.query(project=project, names=['snpeff_hg38_grch38.tgz'])[0]\n",
    "    ref_dict['tool_name'] = tool_name\n",
    "    # db file name list has optional databases to run\n",
    "#     if len(vcf_list) > 0:\n",
    "#         ref_dict['db_vcfs'] = []\n",
    "#         for vcf in vcf_list:\n",
    "#             ref_dict['db_vcfs'].append(api.files.query(project=project, names=[vcf])[0])\n",
    "#     if gwas_bool:\n",
    "#         ref_dict['gwas_catalog_txt'].append(api.files.query(project=project, names=['gwas_catalog_v1.0-associations_e98_r2020-03-08.tsv'])[0])\n",
    "#     if dbnsfp_txt_bool:\n",
    "#         ref_dict['dbnsfp_txt'].append(api.files.query(project=project, names=['dbNSFP4.0a.gz'])[0])\n",
    "    return ref_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all vars\n",
    "app = project + \"/snpeff-annotate\"\n",
    "manifest = open(snp_rm_vcf_manifest)\n",
    "task_prefix = 'snpEff NO DB NO SNP refGene: '\n",
    "# hg38 or GRCh38.86\n",
    "ref_gene_model = \"hg38\"\n",
    "# run gwas?\n",
    "# gwas_bool = False\n",
    "# run dbnsfp?\n",
    "# dbnsfp_txt_bool = False\n",
    "tool_name = \"gatk.denovo.trio.stripped\"\n",
    "\n",
    "ref_obj = get_snpEff_refs(ref_gene_model, tool_name)\n",
    "head = next(manifest)\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_vcf = api.files.get(info[0])\n",
    "    task_name = task_prefix + in_vcf.metadata['Kids First Family ID']\n",
    "    in_dict['input_vcf'] = in_vcf\n",
    "    draft_task(task_name, in_dict, app, project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up annovar run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ANNOVAR_refs(db_list, db_run_bool, protocol_name):\n",
    "    ref_dict = {}\n",
    "    ref_dict['run_dbs'] = db_run_bool\n",
    "    ref_dict['cache'] = api.files.query(project=project, names=['annovar_2019Oct24.tgz'])[0]\n",
    "    ref_dict['protocol_name'] = protocol_name\n",
    "    # db file name list has optional databases to run\n",
    "    if len(db_list) > 0:\n",
    "        ref_dict['additional_dbs'] = []\n",
    "        for db in db_list:\n",
    "            ref_dict['additional_dbs'].append(api.files.query(project=project, names=[db])[0])\n",
    "    return ref_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = project + \"/kfdrc-annovar\"\n",
    "manifest = open(snp_rm_vcf_manifest)\n",
    "task_prefix = 'ANNOVAR NO DB NO SNP refGene: '\n",
    "additional_dbs = ['esp6500siv2_all.tgz']\n",
    "db_run_bool = False\n",
    "# choices are refGene, ensGene, knownGene\n",
    "protocol_name = 'refGene'\n",
    "\n",
    "\n",
    "ref_obj = get_ANNOVAR_refs(additional_dbs, db_run_bool, protocol_name)\n",
    "head = next(manifest)\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_vcf = api.files.get(info[0])\n",
    "    task_name = task_prefix + in_vcf.metadata['Kids First Family ID']\n",
    "    in_dict['input_vcf'] = in_vcf\n",
    "    draft_task(task_name, in_dict, app, project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up VEP run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VEP_refs(db_key_list, db_run_bool):\n",
    "    extra_db_dict = {'cadd_indels': 'CADDv1.5-38-InDels.tsv.gz', 'cadd_snvs': 'CADDv1.5-38-whole_genome_SNVs.tsv.gz',\n",
    "                     'dbnsfp': 'dbNSFP4.0a.gz', 'dbscsnv': 'dbscSNV1.1_GRCh38.txt.gz', 'phylop': 'hg38.phyloP100way.bw'}\n",
    "    ref_dict = {}\n",
    "    ref_dict['run_cache_dbs'] = db_run_bool\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens.GRCh38.dna.toplevel.fa.gz'])[0]\n",
    "    ref_dict['cache'] = api.files.query(project=project, names=['homo_sapiens_merged_vep_99_GRCh38.tar.gz'])[0]\n",
    "    ref_dict['tool_name'] = 'VEP99'\n",
    "    # db_key list has optional databases to run\n",
    "    for key in db_key_list:\n",
    "        ref_dict[key] = api.files.query(project=project, names=[extra_db_dict[key]])[0]\n",
    "    return ref_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = project + \"/kfdrc-vep99-wgsa\"\n",
    "manifest = open(snp_rm_vcf_manifest)\n",
    "task_prefix = 'VEP NO DB NO SNP: '\n",
    "additional_dbs = []\n",
    "db_run_bool = False\n",
    "\n",
    "ref_obj = get_VEP_refs(additional_dbs, db_run_bool)\n",
    "head = next(manifest)\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_vcf = api.files.get(info[0])\n",
    "    task_name = task_prefix + in_vcf.metadata['Kids First Family ID']\n",
    "    in_dict['input_vcf'] = in_vcf\n",
    "    draft_task(task_name, in_dict, app, project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up WGSA run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WGSA_refs(settings, tool_name, db_list):\n",
    "    ref_dict = {}\n",
    "    # db_list = ['dbSNP.tgz', 'GWAS_catalog.tgz', 'wgsa_hg38_resource.tgz', '1000Gp3.tgz', 'UK10K.tgz', 'ESP6500.tgz', 'ExACr0.3.tgz',\n",
    "#                'dbNSFP.tgz', 'CADDv1.4.tgz', 'clinvar.tgz', 'wgsa_hg19_resource.tgz', 'COSMIC_hg38.tgz', 'PhyloP_hg38.tgz', 'gnomAD.tgz',\n",
    "#                'crossmap.tgz']\n",
    "    ref_dict['tool_name'] = tool_name\n",
    "    ref_dict['annovar_ref'] = api.files.query(project=project, names=['annovar_2019Oct24.tgz'])[0]\n",
    "    ref_dict['snpeff_ref'] = api.files.query(project=project, names=['snpeff_hg38_grch38.tgz'])[0]\n",
    "    ref_dict['VEP_cache'] = api.files.query(project=project, names=['homo_sapiens_merged_vep_99_GRCh38.tar.gz'])[0]\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens.GRCh38.dna.toplevel.fa.gz'])[0]\n",
    "    ref_dict['settings'] = api.files.query(project=project, names=[settings])[0]\n",
    "    # db file name list has optional databases to run\n",
    "    if len(db_list) > 0:\n",
    "        ref_dict['resources'] = []\n",
    "        for db in db_list:\n",
    "            ref_dict['resources'].append(api.files.query(project=project, names=[db])[0])\n",
    "    return ref_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = project + \"/kfdrc-wgsa-annotate\"\n",
    "manifest = open(stripped_vcf_manifest)\n",
    "task_prefix = 'WGSA SNP INDEL ALL RPT: '\n",
    "tool_name = \"gatk.denovo.trio.stripped.all_annot\"\n",
    "settings = \"wgsa_all_desired_settings.txt\"\n",
    "# settings = \"WGSA_indel_only_ALL.txt\"\n",
    "db_list = ['precomputed_hg38.tgz', 'dbSNP.tgz', 'GWAS_catalog.tgz', 'wgsa_hg38_resource.tgz', '1000Gp3.tgz', 'UK10K.tgz', 'ESP6500.tgz', 'ExACr0.3.tgz',\n",
    "           'dbNSFP.tgz', 'CADDv1.4.tgz', 'clinvar.tgz', 'wgsa_hg19_resource.tgz', 'COSMIC_hg38.tgz', 'PhyloP_hg38.tgz', 'gnomAD.tgz',\n",
    "           'crossmap.tgz']\n",
    "\n",
    "ref_obj = get_WGSA_refs(settings, tool_name, db_list)\n",
    "head = next(manifest)\n",
    "\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_vcf = api.files.get(info[0])\n",
    "    task_name = task_prefix + in_vcf.metadata['Kids First Family ID']\n",
    "    in_dict['input_vcf'] = in_vcf\n",
    "    draft_task(task_name, in_dict, app, project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up all caller + db run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caller_db_refs(tool_name, strip_info, filter_vcf):\n",
    "    ref_dict = {}\n",
    "    ref_dict['tool_name'] = tool_name\n",
    "    ref_dict['strip_info'] = strip_info\n",
    "    if filter_vcf:\n",
    "        ref_dict['include_expression'] = filter_vcf\n",
    "    ref_dict['ANNOVAR_cache'] = api.files.query(project=project, names=['annovar_2019Oct24.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_cosmic_db'] = api.files.query(project=project, names=['cosmic90.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_dbscsnv_db'] = api.files.query(project=project, names=['dbscsnv11.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_kg_db'] = api.files.query(project=project, names=['1000g2015aug.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_esp_db'] = api.files.query(project=project, names=['esp6500siv2_all.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_gnomad_db'] = api.files.query(project=project, names=['gnomad30_genome.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_run_dbs_refGene'] = True\n",
    "    ref_dict['ANNOVAR_run_dbs_ensGene'] = False\n",
    "    ref_dict['ANNOVAR_run_dbs_knownGene'] = False\n",
    "    ref_dict['snpEff_ref_tar_gz'] = api.files.query(project=project, names=['snpeff_hg38_grch38.tgz'])[0]\n",
    "    ref_dict['gwas_cat_db_file'] = api.files.query(project=project, names=['gwas_catalog_v1.0-associations_e98_r2020-03-08.tsv'])[0]\n",
    "    ref_dict['SnpSift_vcf_db_name'] = \"ClinVar\"\n",
    "    ref_dict['SnpSift_vcf_fields'] = \"AF_ESP,AF_EXAC,AF_TGP,ALLELEID,CLNDN,CLNDNINCL,CLNDISDB,CLNDISDBINCL,CLNHGVS,CLNREVSTAT,CLNSIG,CLNSIGCONF,CLNSIGINCL,CLNVC,CLNVCSO,CLNVI,DBVARID,GENEINFO,MC,ORIGIN,RS,SSR\"\n",
    "    ref_dict['clinvar_vcf'] = api.files.query(project=project, names=['clinvar-2020-03-17.vcf.gz'])[0]\n",
    "    ref_dict['VEP_cache'] = api.files.query(project=project, names=['homo_sapiens_merged_vep_99_GRCh38.tar.gz'])[0]\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens.GRCh38.dna.toplevel.fa.gz'])[0]\n",
    "    ref_dict['VEP_run_cache_existing'] = True\n",
    "    ref_dict['VEP_run_cache_af'] = True\n",
    "    ref_dict['VEP_cadd_indels'] = api.files.query(project=project, names=['CADDv1.5-38-InDels.tsv.gz'])[0]\n",
    "    ref_dict['VEP_cadd_snvs'] = api.files.query(project=project, names=['CADDv1.5-38-whole_genome_SNVs.tsv.gz'])[0]\n",
    "    ref_dict['VEP_dbnsfp'] = api.files.query(project=project, names=['dbNSFP4.0a.gz'])[0]\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = project + \"/kf-caller-db-wf\"\n",
    "manifest = open(subset_test)\n",
    "task_prefix = 'ALL CALLER W DB NO SNP TEST: '\n",
    "tool_name = \"gatk.denovo.trio\"\n",
    "strip_info = \"INFO/ANN\"\n",
    "filter_vcf = \"TYPE!=\\\"snp\\\"\"\n",
    "# db_list = ['cosmic90.tgz', 'dbscsnv11.tgz','gnomad30_genome.tgz']\n",
    "ref_obj = get_caller_db_refs(tool_name, strip_info, filter_vcf)\n",
    "head = next(manifest)\n",
    "\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_vcf = api.files.get(info[0])\n",
    "    task_name = task_prefix + in_vcf.metadata['Kids First Family ID']\n",
    "    in_dict['input_vcf'] = in_vcf\n",
    "    draft_task(task_name, in_dict, app, project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up caller only run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_caller_only_refs(tool_name, strip_info, filter_vcf):\n",
    "    ref_dict = {}\n",
    "    ref_dict['tool_name'] = tool_name\n",
    "    ref_dict['strip_info'] = strip_info\n",
    "    if filter_vcf:\n",
    "        ref_dict['include_expression'] = filter_vcf\n",
    "    ref_dict['ANNOVAR_cache'] = api.files.query(project=project, names=['annovar_2019Oct24.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_run_dbs_refGene'] = False\n",
    "    ref_dict['ANNOVAR_run_dbs_ensGene'] = False\n",
    "    ref_dict['ANNOVAR_run_dbs_knownGene'] = False\n",
    "    ref_dict['snpEff_ref_tar_gz'] = api.files.query(project=project, names=['snpeff_hg38_grch38.tgz'])[0]\n",
    "    ref_dict['VEP_cache'] = api.files.query(project=project, names=['homo_sapiens_merged_vep_99_GRCh38.tar.gz'])[0]\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens.GRCh38.dna.toplevel.fa.gz'])[0]\n",
    "    ref_dict['VEP_run_cache_existing'] = False\n",
    "    ref_dict['VEP_run_cache_af'] = False\n",
    "    return ref_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = project + \"/kf-caller-only-wf\"\n",
    "manifest = open(subset_test)\n",
    "task_prefix = 'ALL CALLER ONLY NO SNP TEST: '\n",
    "tool_name = \"gatk.denovo.trio\"\n",
    "strip_info = \"INFO/ANN\"\n",
    "filter_vcf = \"TYPE!=\\\"snp\\\"\"\n",
    "ref_obj = get_caller_only_refs(tool_name, strip_info, filter_vcf)\n",
    "head = next(manifest)\n",
    "\n",
    "for line in manifest:\n",
    "    info = line.split(',')\n",
    "    in_dict = {}\n",
    "    for key in ref_obj:\n",
    "        in_dict[key] = ref_obj[key]\n",
    "    in_vcf = api.files.get(info[0])\n",
    "    task_name = task_prefix + in_vcf.metadata['Kids First Family ID']\n",
    "    in_dict['input_vcf'] = in_vcf\n",
    "    draft_task(task_name, in_dict, app, project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple utility to expand ipython view in browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:75% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Task Cost/Run analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_sum_time_intervals(task):\n",
    "    data = []\n",
    "    for job in task.get_execution_details().jobs:\n",
    "        if job.status != \"COMPLETED\":\n",
    "            sys.stderr.write(\"Skipping job likely killed due to spot instance kill for \" + job.name + \" from task \" + task.id + \"\\n\")\n",
    "        else:\n",
    "            try:\n",
    "                pair = (job.start_time, job.end_time)\n",
    "                data.append(pair)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "                pdb.set_trace()\n",
    "                hold = 1\n",
    "    data = sorted(data, key=lambda x: x[0])\n",
    "\n",
    "\n",
    "    # from https://stackoverflow.com/questions/34797525/how-to-correctly-merge-overlapping-datetime-ranges-in-python\n",
    "    result = []\n",
    "    try:\n",
    "        t_old = data[0]\n",
    "        for t in data[1:]:\n",
    "            if t_old[1] >= t[0]:  #I assume that the data is sorted already\n",
    "                t_old = ((min(t_old[0], t[0]), max(t_old[1], t[1])))\n",
    "            else:\n",
    "                result.append(t_old)\n",
    "                t_old = t\n",
    "\n",
    "        else:\n",
    "            result.append(t_old)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pdb.set_trace()\n",
    "        hold = 1\n",
    "    total_seconds = 0\n",
    "    for t_int in result:\n",
    "        total_seconds += (t_int[1] - t_int[0]).seconds\n",
    "    #print ('Task ran in ' + str(total_seconds) + ', which is ' + str(total_seconds/3600) + ' hours')\n",
    "    return [task.id, task.name, str(total_seconds/3600)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(task):\n",
    "    try:\n",
    "        return [task.id, task.name, str(task.price.amount), task.app]\n",
    "    except Exception as e:\n",
    "        sys.stderr.write('Could not process task ' + task.id + ' ' + task.name + ' got error ' + str(e) + '\\n')\n",
    "        sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"^WGSA\"\n",
    "task_summary = open('/Users/brownm28/Documents/2020-Mar-4_WGSA/TASK_RUN/WGSA_test,txt', 'w')\n",
    "task_summary.write(\"task_id\\ttask_name\\trun_hrs\\tcost\\tworkflow\\n\")\n",
    "for task in api.tasks.query(project=project, status=\"COMPLETED\").all():\n",
    "    if re.search(task_prefix, task.name):\n",
    "        (rid, rname, duration) = merge_and_sum_time_intervals(task)\n",
    "        (cid, cname, cost, wf) = get_cost(task)\n",
    "        task_summary.write(\"\\t\".join([rid, rname, duration, cost, wf]) + \"\\n\")\n",
    "task_summary.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all task outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_manifest(out_fh, file_obj, out_key, task_name):\n",
    "    out_fh.write(\",\".join([file_obj.id, file_obj.name, out_key, task_name]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing task: KF ANNOT UNK VAR: FM_CN3QK5A1\n",
      "Processing task: KF ANNOT UNK VAR: FM_DFW18WG8\n",
      "Processing task: KF ANNOT UNK VAR: FM_0ADXQA3J\n",
      "Processing task: KF ANNOT UNK VAR: FM_0DB3211J\n",
      "Processing task: KF ANNOT UNK VAR: FM_ZFMCW3G3\n",
      "Processing task: KF ANNOT UNK VAR: FM_SHJNRP8S\n",
      "Processing task: KF ANNOT UNK VAR: FM_4Y21X6PP\n",
      "Processing task: KF ANNOT UNK VAR: FM_0FJQBB97\n",
      "Processing task: KF ANNOT UNK VAR: FM_RT6VGNSJ\n",
      "Processing task: KF ANNOT UNK VAR: FM_1PCY4Y30\n",
      "Processing task: KF ANNOT UNK VAR: FM_V9TXBYS6\n",
      "Processing task: KF ANNOT UNK VAR: FM_11ASN8XN\n",
      "Processing task: KF ANNOT UNK VAR: FM_DKX82ZD7\n",
      "Processing task: KF ANNOT UNK VAR: FM_3FWV6NG5\n",
      "Processing task: KF ANNOT UNK VAR: FM_ZETA86QZ\n",
      "Processing task: KF ANNOT UNK VAR: FM_PR07SNT9\n",
      "Processing task: KF ANNOT UNK VAR: FM_2C8A094S\n",
      "Processing task: KF ANNOT UNK VAR: FM_0RWXQH9X\n",
      "Processing task: KF ANNOT UNK VAR: FM_NH1CE0E4\n",
      "Processing task: KF ANNOT UNK VAR: FM_E6EA6D9M\n",
      "Processing task: KF ANNOT UNK VAR: FM_1046D86B\n",
      "Processing task: KF ANNOT UNK VAR: FM_D88974QD\n",
      "Processing task: KF ANNOT UNK VAR: FM_2JDK8NTW\n",
      "Processing task: KF ANNOT UNK VAR: FM_SSD4PZ41\n",
      "Processing task: KF ANNOT UNK VAR: FM_BNR1K97K\n",
      "Processing task: KF ANNOT UNK VAR: FM_1PEC3KD8\n",
      "Processing task: KF ANNOT UNK VAR: FM_RNRRE1RE\n",
      "Processing task: KF ANNOT UNK VAR: FM_NQ42QZ72\n",
      "Processing task: KF ANNOT UNK VAR: FM_TDNNSVFK\n",
      "Processing task: KF ANNOT UNK VAR: FM_J2AZVX9X\n",
      "Processing task: KF ANNOT UNK VAR: FM_ZEH279Z7\n"
     ]
    }
   ],
   "source": [
    "#test= api.files.query(project=project, metadata={'task_id':'fdb0d85a-2005-4d3e-a3cf-43fb735d13de'}).all()\n",
    "tasks = api.tasks.query(project=project, status=\"COMPLETED\").all()\n",
    "out = open('/Users/brownm28/Documents/2020-Mar-4_WGSA/EXPORT/unk_indel_vcf_manifest.csv', 'w')\n",
    "out.write(\"id,name,output_category,task_name\\n\")\n",
    "phrase = \"KF ANNOT UNK VAR\"\n",
    "for task in tasks:\n",
    "    if re.search(phrase, task.name):\n",
    "        sys.stderr.write('Processing task: ' + task.name + \"\\n\")\n",
    "        for out_key in task.outputs:\n",
    "#             try:\n",
    "            if type(task.outputs[out_key]) is not list:\n",
    "                file_obj = task.outputs[out_key]\n",
    "                write_to_manifest(out, file_obj, out_key, task.name)\n",
    "                if task.outputs[out_key].secondary_files is not None:\n",
    "                    write_to_manifest(out, task.outputs[out_key].secondary_files[0], out_key, task.name)\n",
    "            else:\n",
    "                for i in range(len(task.outputs[out_key])):\n",
    "                    if type(task.outputs[out_key][i]) is not list:\n",
    "                        write_to_manifest(out, task.outputs[out_key][i], out_key, task.name)\n",
    "                        if task.outputs[out_key][i].secondary_files is not None:\n",
    "                            write_to_manifest(out, task.outputs[out_key][i].secondary_files[0], out_key, task.name)\n",
    "                    else:\n",
    "                        for j in range(len(task.outputs[out_key][i])):\n",
    "                            if task.outputs[out_key][i][j] is not None:\n",
    "                                write_to_manifest(out, task.outputs[out_key][i][j], out_key, task.name)\n",
    "                                if task.outputs[out_key][i][j].secondary_files is not None:\n",
    "                                    write_to_manifest(out, task.outputs[out_key][i][j].secondary_files[0], out_key, task.name)\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#                 print(\"Skipping \" + out_key + \" for \" + task.name + \" due to error\")\n",
    "out.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add file metadata to manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = open('/Users/brownm28/Documents/2020-Mar-4_WGSA/EXPORT/faux_vcf_manifest.csv')\n",
    "head = next(manifest)\n",
    "head = head.rstrip('\\n')\n",
    "out = open('/Users/brownm28/Documents/2020-Mar-4_WGSA/EXPORT/exported_vcfs.csv', 'w')\n",
    "out.write(head)\n",
    "\n",
    "init = next(manifest)\n",
    "info = init.rstrip('\\n').split(',')\n",
    "fobj = api.files.get(info[0])\n",
    "keys = []\n",
    "temp = ','.join(info)\n",
    "for key in fobj.metadata:\n",
    "    temp += \",\" + fobj.metadata[key]\n",
    "    keys.append(key)\n",
    "#pdb.set_trace()\n",
    "out.write(\",\" + \",\".join(keys) + \"\\n\" + temp + \"\\n\")\n",
    "for line in manifest:\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    out.write(\",\".join(info))\n",
    "    fobj = api.files.get(info[0])\n",
    "    if fobj.metadata is None:\n",
    "        out.write(\",,\\n\")\n",
    "    else:\n",
    "        for key in fobj.metadata:\n",
    "            if key not in keys:\n",
    "                sys.stderr.write(\"ERROR: \" + key + \" not in keys, rethink strategy for file \" + fobj.name + \"\\n\")\n",
    "                pdb.set_trace()\n",
    "                exit(1)\n",
    "            else:\n",
    "                out.write(\",\" + fobj.metadata[key])\n",
    "        out.write(\"\\n\")\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add file tags to taks outputs\n",
    "Use output from [Get all task outputs](##-Get-all-task-outputs) and add a csv column with desired tags to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You sure want to tag these files using this manifest? Type \"YASS\" if so\n",
      "YASS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 50 files\n"
     ]
    }
   ],
   "source": [
    "manifest = open('/Users/brownm28/Documents/2020-Mar-4_WGSA/EXPORT/missed_tbi.txt')\n",
    "head = next(manifest)\n",
    "header = head.rstrip('\\n').split('\\t')\n",
    "tag_col_name = \"TAG_TO_ADD\"\n",
    "idx = header.index(tag_col_name)\n",
    "print(\"You sure want to tag these files using this manifest? Type \\\"YASS\\\" if so\")\n",
    "check = input()\n",
    "x = 1\n",
    "m = 50\n",
    "if check == \"YASS\":\n",
    "    for line in manifest:\n",
    "        if x % m == 0:\n",
    "            sys.stderr.write(\"Processed \" + str(x) + \" files\\n\")\n",
    "        info = line.rstrip('\\n').split('\\t')\n",
    "        file_obj = api.files.get(info[0])\n",
    "        try:\n",
    "            tags = info[idx].split(',')\n",
    "            file_obj.tags = tags\n",
    "            file_obj.save()\n",
    "        except Exception as e:\n",
    "            sys.stderr.write(str(e) + \"\\nCould not tag file with ID: \" + info[0] + \", skipping!\\n\")\n",
    "        x += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split WF Runs\n",
    "For creating precomputed annotations on simulated SNPs and known indels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_split_task(task_name, input_dict, app_name, project, annotator):\n",
    "    task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=input_dict, run=False)\n",
    "    task.inputs['output_basename'] = task.id + \"_\" + annotator\n",
    "    task.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run VEP Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vep_split_refs(run_norm_bool, tool_name):\n",
    "    ref_dict = {}\n",
    "    # may need to change bed\n",
    "    ref_dict['scatter_bed'] = api.files.query(project=project, names=['ucsc_indel_regions.bed'])[0]\n",
    "#     UNCOMMENT BELOW If INPUT IS MASSIVE SNP FILE\n",
    "#     ref_dict['scatter_ct'] = 200\n",
    "#     ref_dict['bands'] = 1000000\n",
    "#     ref_dict['ram'] = 72\n",
    "#     ref_dict['cores'] = 36\n",
    "    ref_dict['run_vt_norm'] = run_norm_bool\n",
    "    ref_dict['tool_name'] = tool_name\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['reference_dict'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dict'])[0]\n",
    "    ref_dict['VEP_cache'] = api.files.query(project=project, names=['homo_sapiens_merged_vep_99_GRCh38.tar.gz'])[0]\n",
    "    ref_dict['VEP_buffer_size'] = 20000\n",
    "    ref_dict['VEP_run_cache_af'] = False\n",
    "    ref_dict['VEP_run_cache_existing'] = False\n",
    "    ref_dict['VEP_run_stats'] = False\n",
    "    return ref_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vcf = api.files.get('5ec57ff9e4b05495c9a85be8')\n",
    "task_name = \"KFDRC VEP Precomputed: DBSNP V153 non snps\"\n",
    "run_norm_bool = False\n",
    "tool_name = \"DBSNP_SNP_RM_V153\"\n",
    "annotator = \"VEP\"\n",
    "app_name = project + \"/kf-vep99-split-sub-wf\"\n",
    "in_dict = {}\n",
    "ref_objs = get_vep_split_refs(run_norm_bool, tool_name)\n",
    "for key in ref_objs:\n",
    "    in_dict[key] = ref_objs[key]\n",
    "in_dict['input_vcf'] = in_vcf\n",
    "draft_split_task(task_name, in_dict, app_name, project, annotator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run snpEff Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snpEff_refs(reference_name, tool_name):\n",
    "    ref_dict = {}\n",
    "    # may need to change bed\n",
    "    ref_dict['scatter_bed'] = api.files.query(project=project, names=['ucsc_indel_regions.bed'])[0]\n",
    "    ref_dict['snpeff_ref_name'] = ['hg38kg', 'hg38', 'GRCh38.86']\n",
    "    ref_dict['snpEff_ref_tar_gz'] = api.files.query(project=project, names=['snpeff_hg38_hg38kg_grch38.tgz'])[0]\n",
    "    ref_dict['wf_tool_name'] = tool_name\n",
    "#     UNCOMMENT BELOW If INPUT IS MASSIVE SNP FILE\n",
    "#     ref_dict['scatter_ct'] = 200\n",
    "#     ref_dict['bands'] = 1000000\n",
    "#     ref_dict['ram'] = 72\n",
    "#     ref_dict['cores'] = 36\n",
    "    ref_dict['run_vt_norm'] = run_norm_bool\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['reference_dict'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dict'])[0]\n",
    "    return ref_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vcf = api.files.get('5ec57ff9e4b05495c9a85be8')\n",
    "task_name = \"KFDRC snpEff Precomputed: DBSNP V153 non snps\"\n",
    "run_norm_bool = False\n",
    "tool_name = \"DBSNP_SNP_RM_V153\"\n",
    "annotator = \"snpEff\"\n",
    "app_name = project + \"/kf-snp-eff-split-sub-wf\"\n",
    "in_dict = {}\n",
    "ref_objs = get_snpEff_refs(run_norm_bool, tool_name)\n",
    "for key in ref_objs:\n",
    "    in_dict[key] = ref_objs[key]\n",
    "in_dict['input_vcf'] = in_vcf\n",
    "draft_split_task(task_name, in_dict, app_name, project, annotator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ANNOVAR Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annovar_refs(reference_name, tool_name):\n",
    "    ref_dict = {}\n",
    "    # may need to change bed\n",
    "    ref_dict['scatter_bed'] = api.files.query(project=project, names=['ucsc_indel_regions.bed'])[0]\n",
    "    ref_dict['protocol_list'] = ['refGene', 'knownGene', 'ensGene']\n",
    "    ref_dict['ANNOVAR_cache'] = api.files.query(project=project, names=['annovar_2019Oct24.tgz'])[0]\n",
    "    ref_dict['wf_tool_name'] = tool_name\n",
    "    ref_dict['run_dbs'] = [False]\n",
    "#     UNCOMMENT BELOW If INPUT IS MASSIVE SNP FILE\n",
    "#     ref_dict['scatter_ct'] = 200\n",
    "#     ref_dict['bands'] = 1000000\n",
    "#     ref_dict['ram'] = 128\n",
    "#     ref_dict['cores'] = 40\n",
    "    ref_dict['run_vt_norm'] = run_norm_bool\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    ref_dict['reference_dict'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dict'])[0]\n",
    "    return ref_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vcf = api.files.get('5ec57ff9e4b05495c9a85be8')\n",
    "task_name = \"KFDRC ANNOVAR Precomputed: DBSNP V153 non snps\"\n",
    "run_norm_bool = False\n",
    "tool_name = \"DBSNP_SNP_RM_V153\"\n",
    "annotator = \"ANNOVAR\"\n",
    "app_name = project + \"/kf-annovar-w-preprocess-sub-wf\"\n",
    "in_dict = {}\n",
    "ref_objs = get_annovar_refs(run_norm_bool, tool_name)\n",
    "for key in ref_objs:\n",
    "    in_dict[key] = ref_objs[key]\n",
    "in_dict['input_vcf'] = in_vcf\n",
    "draft_split_task(task_name, in_dict, app_name, project, annotator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unknown indel annotation\n",
    "Workflow to take a vcf, normalize it, filter on unkown indels, and annotate. Outputs are normalized vcf and annotated unknown indel results from ANNOVAR, VEP, snpEff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draft_unk_task(task_name, input_dict, app_name, project):\n",
    "    task = api.tasks.create(name=task_name, project=project, app=app_name, inputs=input_dict, run=False)\n",
    "    task.inputs['output_basename'] = task.id\n",
    "    task.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run unknown annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annot_refs(tool_name, reference_vcf, run_norm_bool, strip_info, include_expression):\n",
    "    ref_dict = {}\n",
    "    # general - required\n",
    "    ref_dict['reference_vcf'] = reference_vcf\n",
    "    ref_dict['tool_name'] = tool_name\n",
    "    # general - optional\n",
    "    if run_norm_bool:\n",
    "        ref_dict['run_norm_flag'] = run_norm_bool\n",
    "    if strip_info:\n",
    "        ref_dict['strip_info'] = strip_info\n",
    "    if include_expression:\n",
    "        ref_dict['include_expression'] = include_expression\n",
    "    ref_dict['reference'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.fasta'])[0]\n",
    "    # ref_dict['reference_dict'] = api.files.query(project=project, names=['Homo_sapiens_assembly38.dict'])[0]\n",
    "    # VEP specific\n",
    "    ref_dict['VEP_cache'] = api.files.query(project=project, names=['homo_sapiens_merged_vep_99_GRCh38.tar.gz'])[0]\n",
    "    ref_dict['VEP_buffer_size'] = 20000\n",
    "    ref_dict['VEP_run_cache_af'] = False\n",
    "    ref_dict['VEP_run_cache_existing'] = False\n",
    "    ref_dict['VEP_run_stats'] = False\n",
    "    # ANNOVAR specific\n",
    "    ref_dict['protocol_list'] = ['refGene', 'knownGene', 'ensGene']\n",
    "    ref_dict['ANNOVAR_cache'] = api.files.query(project=project, names=['annovar_2019Oct24.tgz'])[0]\n",
    "    ref_dict['ANNOVAR_run_dbs'] = [False]\n",
    "    # snpEff specific\n",
    "    ref_dict['snpeff_ref_name'] = ['hg38kg', 'hg38', 'GRCh38.86']\n",
    "    ref_dict['snpEff_ref_tar_gz'] = api.files.query(project=project, names=['snpeff_hg38_hg38kg_grch38.tgz'])[0]\n",
    "    return ref_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vcf_manifest = '/Users/brownm28/Documents/2020-Mar-4_WGSA/TASK_RUN/unk_test_set-manifest.csv'\n",
    "app_name = project + \"/kf-unknown-var-annot-wf\"\n",
    "ref_vcf = api.files.query(project=project, names=['dbSNP_v153_ucsc-compatible.normalized.snps_rm.vcf.gz'])[0]\n",
    "# change if somatic\n",
    "tool_name = 'gatk'\n",
    "run_norm = True\n",
    "# for germline calls, probably INFO/ANN, somatic, probably INFO/CSQ\n",
    "strip_info = \"INFO/ANN\"\n",
    "include_expression = 'TYPE!=\"snp\"'\n",
    "vcf_list = open(in_vcf_manifest)\n",
    "head = next(vcf_list)\n",
    "header = head.rstrip('\\n').split(',')\n",
    "# get col of desired ID to ass to task name\n",
    "id_label = \"Kids First Family ID\"\n",
    "idx = header.index(id_label)\n",
    "ref_objs = get_annot_refs(tool_name, ref_vcf, run_norm, strip_info, include_expression) \n",
    "for line in vcf_list:\n",
    "    in_dict = {}\n",
    "    # pdb.set_trace()\n",
    "    for key in ref_objs:\n",
    "        in_dict[key] = ref_objs[key]\n",
    "    info = line.rstrip('\\n').split(',')\n",
    "    task_name = \"KF ANNOT UNK VAR: \" + info[idx]\n",
    "    in_dict['input_vcf'] = api.files.get(info[0])\n",
    "    draft_unk_task(task_name, in_dict, app_name, project)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = api.tasks.get('e3f66229-7217-4ec0-9d76-341b5a772226')\n",
    "pdb.set_trace()\n",
    "hold = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
